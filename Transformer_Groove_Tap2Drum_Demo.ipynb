{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-Groove-Tap2Drum-Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAnqU4Au0hdnq/A8yZ0ieJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinaniet0/TransformerGrooveTap2Drum/blob/main/Transformer_Groove_Tap2Drum_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_BBV218LWCF"
      },
      "source": [
        "# ![Tap2Drum](https://raw.githubusercontent.com/marinaniet0/TransformerGrooveTap2Drum/main/imgs/t2d.png) with **Transformer Neural Networks** - Demo\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEG3UNCoQoaq"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pt3b8qznRE_J"
      },
      "source": [
        "#@title Setup\n",
        "\n",
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "# Cloning repository\n",
        "!git clone --quiet https://github.com/marinaniet0/TransformerGrooveTap2Drum.git\n",
        "\n",
        "# Unzipping dependencies\n",
        "!unzip -qq TransformerGrooveTap2Drum/dependencies.zip -d .\n",
        "\n",
        "# Installing magenta (for note_seq)\n",
        "!pip install -U -q magenta\n",
        "\n",
        "# Getting wandb\n",
        "!pip install -q wandb\n",
        "\n",
        "# Installing fluidsynth\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library\n",
        "\n",
        "# Installing and activating environment\n",
        "#!conda env create -f TransformerGrooveTap2Drum/environment.yml\n",
        "\n",
        "from google.colab import files\n",
        "import IPython.display\n",
        "from IPython.display import Audio\n",
        "import magenta\n",
        "import note_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt9JtjqRRxgc"
      },
      "source": [
        "## MIDI Tap2Drum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gJxmFGLmR2Ly"
      },
      "source": [
        "#@title Import libraries and define util functions\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import note_seq\n",
        "import pretty_midi as pm\n",
        "import copy\n",
        "import wandb\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "sys.path.insert(1, \"/content/BaseGrooveTransformers/\")\n",
        "sys.path.insert(1, \"/content/hvo_sequence/\")\n",
        "\n",
        "from models.train import *\n",
        "from models.transformer import GrooveTransformerEncoder\n",
        "from hvo_sequence.drum_mappings import ROLAND_REDUCED_MAPPING\n",
        "from hvo_sequence.io_helpers import note_sequence_to_hvo_sequence\n",
        "from hvo_sequence.hvo_seq import empty_like\n",
        "\n",
        "def play(hvo_seq, sf2_path='/content/hvo_sequence/hvo_sequence/soundfonts/Standard_Drum_Kit.sf2'):\n",
        "  audio_seq = hvo_seq.synthesize(sr=44100, sf_path=sf2_path)\n",
        "  IPython.display.display(IPython.display.Audio(audio_seq, rate=44100))\n",
        "\n",
        "def fixed_hvo_tsteps(hvo_arr, n_tsteps):\n",
        "  if hvo_arr.shape[0] > n_tsteps:\n",
        "    _hvo_arr = hvo_arr[:n_tsteps,:]\n",
        "  elif hvo_arr.shape[0] < n_tsteps:\n",
        "    _hvo_arr = np.concatenate((hvo_arr,np.zeros((n_tsteps-hvo_arr.shape[0], hvo_arr.shape[1]))))\n",
        "  else:\n",
        "    _hvo_arr = hvo_arr\n",
        "  return _hvo_arr"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KhDmbNr1xP1U"
      },
      "source": [
        "#@title Tappify your own drum MIDI file or use an example from the Groove MIDI Dataset\n",
        "upload_myown_midi_file = True #@param {type:\"boolean\"}\n",
        "\n",
        "if upload_myown_midi_file:\n",
        "  uploaded_file = files.upload()\n",
        "  FILEPATH = list(uploaded.keys())[0]\n",
        "else:\n",
        "  FILEPATH = '/content/TransformerGrooveTap2Drum/groove_midi_examples/drummer7_session2_123_hiphop_140_beat_4-4.mid'\n",
        "\n",
        "# Getting HVO representation\n",
        "gt_midi = pm.PrettyMIDI(FILEPATH)\n",
        "gt_note_seq = note_seq.midi_to_note_sequence(gt_midi)\n",
        "gt_hvo_seq = note_sequence_to_hvo_sequence(ns=gt_note_seq, drum_mapping=ROLAND_REDUCED_MAPPING)\n",
        "\n",
        "# Taking first 2 bars of file, padding with 0 if necessary\n",
        "gt_hvo_seq.hvo = fixed_hvo_tsteps(gt_hvo_seq.hvo, 32)\n",
        "\n",
        "tap_hvo_seq = copy.deepcopy(example_hvo_seq)\n",
        "tap_hvo_seq.hvo = example_hvo_seq.flatten_voices()\n",
        "\n",
        "print(\"Ground truth:\")\n",
        "play(gt_hvo_seq)\n",
        "print(\"Tappified:\")\n",
        "play(tap_hvo_seq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ixvxZybO1PtW"
      },
      "source": [
        "#@title Choose and load model\n",
        "TRAINED_MODELS_PATH = \"/content/TransformerGrooveTap2Drum/trained_models/\"\n",
        "model_filename = 'misunderstood_bush_246-epoch_26.Model' #@param [\"hopeful_gorge_252-epoch_90.Model\",\"misunderstood_bush_246-epoch_26.Model\", \"rosy_durian_248-epoch_26.Model\", \"solar_shadow_247-epoch_41.Model\"]\n",
        "\n",
        "params = {\n",
        "    'hopeful':{ 'd_model': 512, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                      'dim_ff': 64, 'dropout': 0.1708, 'n_layers': 8,\n",
        "                      'max_len': 32, 'device': 'cpu' },\n",
        "    'misunderstood':{ 'd_model': 128, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                          'dim_ff': 128, 'dropout': 0.1038, 'n_layers': 11,\n",
        "                          'max_len': 32, 'device': 'cpu' },\n",
        "    'rosy':{ 'd_model': 512, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                    'dim_ff': 16, 'dropout': 0.1093, 'n_layers': 6,\n",
        "                    'max_len': 32, 'device': 'cpu' },\n",
        "    'solar':{ 'd_model': 128, 'embedding_sz': 27, 'n_heads': 1,\n",
        "                     'dim_ff': 16, 'dropout': 0.1594, 'n_layers': 7,\n",
        "                     'max_len': 32, 'device': 'cpu' }\n",
        "}\n",
        "\n",
        "selected_model_params = params[model_filename.split('_')[0]]\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(os.path.join(TRAINED_MODELS_PATH, model_filename),\n",
        "                        map_location=torch.device(selected_model_params['device']))\n",
        "\n",
        "# Initialize model\n",
        "groove_transformer = GrooveTransformerEncoder(selected_model_params['d_model'],\n",
        "                                              selected_model_params['embedding_sz'],\n",
        "                                              selected_model_params['embedding_sz'],\n",
        "                                              selected_model_params['n_heads'],\n",
        "                                              selected_model_params['dim_ff'],\n",
        "                                              selected_model_params['dropout'],\n",
        "                                              selected_model_params['n_layers'],\n",
        "                                              selected_model_params['max_len'],\n",
        "                                              selected_model_params['device'])\n",
        "# Load model and put in evaluation mode\n",
        "groove_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "groove_transformer.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8cu0MBIy6oiM"
      },
      "source": [
        "#@title Generate prediction from tapped input\n",
        "\n",
        "# hit_activation = \"use_probability_distribution\" #@param [\"use_threshold\", \"use_probability_distribution\"]\n",
        "hit_activation_threshold = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# tapped sequence to tensor\n",
        "tap_hvo_tensor = torch.FloatTensor(tap_hvo_seq.hvo)\n",
        "\n",
        "\n",
        "#if hit_activation == \"use_threshold\":\n",
        "pred_h, pred_v, pred_o = groove_transformer.predict(\n",
        "  tap_hvo_tensor, use_thres=True, thres=hit_activation_threshold)\n",
        "#else:\n",
        "#  pred_h, pred_v, pred_o = groove_transformer.predict(\n",
        "#    tap_hvo_tensor, use_thres=False, use_pd=True)\n",
        "\n",
        "prediction_hvo_seq = empty_like(tap_hvo_seq)\n",
        "prediction_hvo_seq.hvo = np.zeros((32, 27))\n",
        "prediction_hvo_seq.hits = pred_h.numpy()[0]\n",
        "prediction_hvo_seq.velocities = pred_v.numpy()[0]\n",
        "prediction_hvo_seq.offsets = pred_o.numpy()[0]\n",
        "\n",
        "print(\"Tapped sequence:\")\n",
        "play(tap_hvo_seq)\n",
        "print(\"Generated beat:\")\n",
        "play(prediction_hvo_seq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}